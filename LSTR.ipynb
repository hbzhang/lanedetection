{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTR.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1j4muj5uItePKQtyrFrig2fbHcWBLuDCt","authorship_tag":"ABX9TyNfP0teoHm2VbDH9swM0NyE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IRk_tzG5dv4o","colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"error","timestamp":1613945148326,"user_tz":300,"elapsed":1106,"user":{"displayName":"Hongbo Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirytrelVneNKiZHrfSJ6MPnuJwzIgaziC376Hfg1M=s64","userId":"08451601199128892090"}},"outputId":"5c35303a-15f0-41e2-ad3f-59ed678b88c3"},"source":["#!/usr/bin/env python\n","#from google.colab import drive\n","#drive.mount('/content/drive/MyDrive/Colab-Notebooks/LSTR')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab-Notebooks/LSTR')\n","\n","import os\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","import json\n","import torch\n","import numpy as np\n","import queue\n","import pprint\n","import random\n","import argparse\n","import importlib\n","import threading\n","import traceback\n","\n","from tqdm import tqdm\n","from utils import stdout_to_tqdm\n","from config import system_configs\n","from nnet.py_factory import NetworkFactory\n","from torch.multiprocessing import Process, Queue, Pool\n","from db.datasets import datasets\n","import models.py_utils.misc as utils\n","\n","torch.backends.cudnn.enabled   = True\n","torch.backends.cudnn.benchmark = True\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Train CornerNet\")\n","    parser.add_argument(\"cfg_file\", help=\"config file\", type=str)\n","    parser.add_argument(\"--iter\", dest=\"start_iter\",\n","                        help=\"train at iteration i\",\n","                        default=0, type=int)\n","    parser.add_argument(\"--threads\", dest=\"threads\", default=4, type=int)\n","    parser.add_argument(\"--freeze\", action=\"store_true\")\n","\n","    args = parser.parse_args()\n","    return args\n","\n","def make_dirs(directories):\n","    for directory in directories:\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","\n","def prefetch_data(db, queue, sample_data):\n","    ind = 0\n","    print(\"start prefetching data...\")\n","    np.random.seed(os.getpid())\n","    while True:\n","        try:\n","            data, ind = sample_data(db, ind)\n","            queue.put(data)\n","        except Exception as e:\n","            traceback.print_exc()\n","            raise e\n","\n","def pin_memory(data_queue, pinned_data_queue, sema):\n","    while True:\n","        data = data_queue.get()\n","\n","        data[\"xs\"] = [x.pin_memory() for x in data[\"xs\"]]\n","        data[\"ys\"] = [y.pin_memory() for y in data[\"ys\"]]\n","\n","        pinned_data_queue.put(data)\n","\n","        if sema.acquire(blocking=False):\n","            return\n","\n","def init_parallel_jobs(dbs, queue, fn):\n","    tasks = [Process(target=prefetch_data, args=(db, queue, fn)) for db in dbs]\n","    for task in tasks:\n","        task.daemon = True\n","        task.start()\n","    return tasks\n","\n","def train(training_dbs, validation_db, start_iter=0, freeze=False):\n","    learning_rate    = system_configs.learning_rate\n","    max_iteration    = system_configs.max_iter\n","    pretrained_model = system_configs.pretrain\n","    snapshot         = system_configs.snapshot\n","    val_iter         = system_configs.val_iter\n","    display          = system_configs.display\n","    decay_rate       = system_configs.decay_rate\n","    stepsize         = system_configs.stepsize\n","    batch_size       = system_configs.batch_size\n","\n","    # getting the size of each database\n","    training_size   = len(training_dbs[0].db_inds)\n","    validation_size = len(validation_db.db_inds)\n","\n","    # queues storing data for training\n","    training_queue   = Queue(system_configs.prefetch_size) # 5\n","    validation_queue = Queue(5)\n","\n","    # queues storing pinned data for training\n","    pinned_training_queue   = queue.Queue(system_configs.prefetch_size) # 5\n","    pinned_validation_queue = queue.Queue(5)\n","\n","    # load data sampling function\n","    data_file   = \"sample.{}\".format(training_dbs[0].data) # \"sample.coco\"\n","    sample_data = importlib.import_module(data_file).sample_data\n","    # print(type(sample_data)) # function\n","\n","    # allocating resources for parallel reading\n","    training_tasks   = init_parallel_jobs(training_dbs, training_queue, sample_data)\n","    if val_iter:\n","        validation_tasks = init_parallel_jobs([validation_db], validation_queue, sample_data)\n","\n","    training_pin_semaphore   = threading.Semaphore()\n","    validation_pin_semaphore = threading.Semaphore()\n","    training_pin_semaphore.acquire()\n","    validation_pin_semaphore.acquire()\n","\n","    training_pin_args   = (training_queue, pinned_training_queue, training_pin_semaphore)\n","    training_pin_thread = threading.Thread(target=pin_memory, args=training_pin_args)\n","    training_pin_thread.daemon = True\n","    training_pin_thread.start()\n","\n","    validation_pin_args   = (validation_queue, pinned_validation_queue, validation_pin_semaphore)\n","    validation_pin_thread = threading.Thread(target=pin_memory, args=validation_pin_args)\n","    validation_pin_thread.daemon = True\n","    validation_pin_thread.start()\n","\n","    print(\"building model...\")\n","    nnet = NetworkFactory(flag=True)\n","\n","    if pretrained_model is not None:\n","        if not os.path.exists(pretrained_model):\n","            raise ValueError(\"pretrained model does not exist\")\n","        print(\"loading from pretrained model\")\n","        nnet.load_pretrained_params(pretrained_model)\n","\n","    if start_iter:\n","        learning_rate /= (decay_rate ** (start_iter // stepsize))\n","\n","        nnet.load_params(start_iter)\n","        nnet.set_lr(learning_rate)\n","        print(\"training starts from iteration {} with learning_rate {}\".format(start_iter + 1, learning_rate))\n","    else:\n","        nnet.set_lr(learning_rate)\n","\n","    print(\"training start...\")\n","    nnet.cuda()\n","    nnet.train_mode()\n","    header = None\n","    metric_logger = utils.MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","    metric_logger.add_meter('class_error', utils.SmoothedValue(window_size=1, fmt='{value:.2f}'))\n","\n","    with stdout_to_tqdm() as save_stdout:\n","        for iteration in metric_logger.log_every(tqdm(range(start_iter + 1, max_iteration + 1),\n","                                                      file=save_stdout, ncols=67),\n","                                                 print_freq=10, header=header):\n","\n","            training = pinned_training_queue.get(block=True)\n","            viz_split = 'train'\n","            save = True if (display and iteration % display == 0) else False\n","            (set_loss, loss_dict) \\\n","                = nnet.train(iteration, save, viz_split, **training)\n","            (loss_dict_reduced, loss_dict_reduced_unscaled, loss_dict_reduced_scaled, loss_value) = loss_dict\n","            metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n","            metric_logger.update(class_error=loss_dict_reduced['class_error'])\n","            metric_logger.update(lr=learning_rate)\n","\n","            del set_loss\n","\n","            if val_iter and validation_db.db_inds.size and iteration % val_iter == 0:\n","                nnet.eval_mode()\n","                viz_split = 'val'\n","                save = True\n","                validation = pinned_validation_queue.get(block=True)\n","                (val_set_loss, val_loss_dict) \\\n","                    = nnet.validate(iteration, save, viz_split, **validation)\n","                (loss_dict_reduced, loss_dict_reduced_unscaled, loss_dict_reduced_scaled, loss_value) = val_loss_dict\n","                print('[VAL LOG]\\t[Saving training and evaluating images...]')\n","                metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)\n","                metric_logger.update(class_error=loss_dict_reduced['class_error'])\n","                metric_logger.update(lr=learning_rate)\n","                nnet.train_mode()\n","\n","            if iteration % snapshot == 0:\n","                nnet.save_params(iteration)\n","\n","            if iteration % stepsize == 0:\n","                learning_rate /= decay_rate\n","                nnet.set_lr(learning_rate)\n","\n","            if iteration % (training_size // batch_size) == 0:\n","                metric_logger.synchronize_between_processes()\n","                print(\"Averaged stats:\", metric_logger)\n","\n","\n","    # sending signal to kill the thread\n","    training_pin_semaphore.release()\n","    validation_pin_semaphore.release()\n","\n","    # terminating data fetching processes\n","    for training_task in training_tasks:\n","        training_task.terminate()\n","    for validation_task in validation_tasks:\n","        validation_task.terminate()\n","\n","if __name__ == \"__main__\":\n","    args = parse_args()\n","\n","    cfg_file = os.path.join(system_configs.config_dir, args.cfg_file + \".json\")\n","    with open(cfg_file, \"r\") as f:\n","        configs = json.load(f)\n","\n","    configs[\"system\"][\"snapshot_name\"] = args.cfg_file  # CornerNet\n","    system_configs.update_config(configs[\"system\"])\n","\n","    train_split = system_configs.train_split\n","    val_split   = system_configs.val_split\n","\n","    dataset = system_configs.dataset  # MSCOCO | FVV\n","    print(\"loading all datasets {}...\".format(dataset))\n","\n","    threads = args.threads  # 4 every 4 epoch shuffle the indices\n","    print(\"using {} threads\".format(threads))\n","    training_dbs  = [datasets[dataset](configs[\"db\"], train_split) for _ in range(threads)]\n","    validation_db = datasets[dataset](configs[\"db\"], val_split)\n","\n","    # print(\"system config...\")\n","    # pprint.pprint(system_configs.full)\n","    #\n","    # print(\"db config...\")\n","    # pprint.pprint(training_dbs[0].configs)\n","\n","    print(\"len of training db: {}\".format(len(training_dbs[0].db_inds)))\n","    print(\"len of testing db: {}\".format(len(validation_db.db_inds)))\n","\n","    print(\"freeze the pretrained network: {}\".format(args.freeze))\n","    train(training_dbs, validation_db, args.start_iter, args.freeze) # 0"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-c6c26148e5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstdout_to_tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msystem_configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetworkFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab-Notebooks/LSTR/nnet/py_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclever_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msystem_configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'thop'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCmg4eWzhNiC","executionInfo":{"status":"ok","timestamp":1613943564156,"user_tz":300,"elapsed":518,"user":{"displayName":"Hongbo Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirytrelVneNKiZHrfSJ6MPnuJwzIgaziC376Hfg1M=s64","userId":"08451601199128892090"}},"outputId":"6389b619-83b8-4da0-b31f-79509c96bdf9"},"source":["! ls /content/drive/MyDrive/Colab-Notebooks/LSTR"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cache\t   db\t\t    LSTR.ipynb\tREADME.md\t  test\t    utils\n","config\t   environment.txt  models\trequirements.txt  test.py\n","config.py  LICENSE\t    nnet\tsample\t\t  train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PowLBC-hd_ER","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613943876581,"user_tz":300,"elapsed":511,"user":{"displayName":"Hongbo Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirytrelVneNKiZHrfSJ6MPnuJwzIgaziC376Hfg1M=s64","userId":"08451601199128892090"}},"outputId":"da2aaed7-a27a-4036-8546-6602e84fc493"},"source":["! ls /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3goXpQhmjrlf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLHny0bljg4j","executionInfo":{"status":"ok","timestamp":1613944076187,"user_tz":300,"elapsed":660,"user":{"displayName":"Hongbo Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirytrelVneNKiZHrfSJ6MPnuJwzIgaziC376Hfg1M=s64","userId":"08451601199128892090"}},"outputId":"ed46f8d0-1e93-4531-d012-85f46f40219e"},"source":["! ls /content/drive/MyDrive/Colab-Notebooks/LSTR"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cache\t   db\t\t    LSTR.ipynb\tREADME.md\t  test\t    utils\n","config\t   environment.txt  models\trequirements.txt  test.py\n","config.py  LICENSE\t    nnet\tsample\t\t  train.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ISYpk_RMmVq9"},"source":["%%bash\n","\n","ls /content/drive/MyDrive/Colab-Notebooks/LSTR/nnet\n","\n","which python\n","python --version\n","\n","echo $PYTHONPATH\n","\n","\n","\n"," MINICONDA_INSTALLER_SCRIPT=Miniconda3-latest-Linux-x86_64.sh\n"," MINICONDA_PREFIX=/usr/local\n","#! wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n","!chmod +x $MINICONDA_INSTALLER_SCRIPT\n","! ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\n","\n","#!conda activate lstr\n","\n","#!conda env create --name lstr --file environment.txt\n","\n","#!pip install -r requirements.txt\n","\n","#from nnet.py_factory import NetworkFactory\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gm4Ea3Bi67CG"},"source":["%%bash\n","\n"," MINICONDA_INSTALLER_SCRIPT=Miniconda3-latest-Linux-x86_64.sh\n"," MINICONDA_PREFIX=/usr/local\n"," wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n","\n"," chmod +x $MINICONDA_INSTALLER_SCRIPT\n"," ./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX\n","\n","\n","conda env list\n","\n","cd /content/drive/MyDrive/Colab-Notebooks/LSTR\n","\n","conda create --name lstr --file environment.txt\n","\n","\n","source activate lstr\n","\n","pip install -r requirements.txt\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eX6EWUpG-GyW"},"source":["#import sys\n","#sys.path\n","\n","%%bash\n","source activate lstr\n","cd /content/drive/MyDrive/Colab-Notebooks/LSTR\n","python train.py LSTR"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jbYHaILBiTJ"},"source":["!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!source activate lstr\n","!pip install GPUtil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKr7sR7cEaCq"},"source":["import tensorflow as tf\n","tf.test.gpu_device_name()\n","\n","#https://rjai.me/posts/google-colab-conda/\n","\n","#!conda list\n","\n","%cd /content/drive/MyDrive/Colab-Notebooks/TuSimple/LSTR\n","\n","%ls\n","\n","#!wget -O /content/drive/MyDrive/Colab-Notebooks/TuSimple/LSTR https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh && bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp /usr/local\n","\n","!bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp /usr/local\n","import sys\n","sys.path.insert(0, \"/usr/local/lib/python3.6/site-packages/\")\n","\n","!conda install -y -q -c conda-forge -c omnia/label/cuda100 -c omnia openmm python=3.6 --force\n","\n","#!conda env list\n","\n","!conda create --name lstr1 --file environment.txt --forfce\n","\n","!source activate lstr1\n","\n","!conda install -c intel mkl_fft --force\n","\n","#!pip install GPUtil\n","\n","\n","\n","%ls \n","#python train.py LSTR\n","\n","\n","%pip install -r requirements.txt --ignore-installed\n","\n","#!conda list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12nkBmvPu3Rg"},"source":["%%bash\n","\n","ls\n","\n","conda env list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPm7jRZYUVCC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614206085330,"user_tz":300,"elapsed":10970416,"user":{"displayName":"Hongbo Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirytrelVneNKiZHrfSJ6MPnuJwzIgaziC376Hfg1M=s64","userId":"08451601199128892090"}},"outputId":"3683a781-d5f5-4363-9010-f274ed91dc13"},"source":["%%bash\n","\n","ls\n","\n","conda env list\n","\n","#%cd /content/drive/MyDrive/Colab-Notebooks/TuSimple/LSTR\n","\n","source activate lstr1\n","\n","#conda list\n","\n","ls\n","\n","cd /content/drive/MyDrive/Colab-Notebooks/TuSimple/LSTR\n","\n","ls\n","\n","pip install -r requirements.txt --ignore-installed\n","\n","python train.py LSTR"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Process is terminated.\n"],"name":"stdout"}]}]}